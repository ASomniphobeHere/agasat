from openai import OpenAI

# Set your OpenAI API key here or load it from an environment variable
openai_api_key = "***REMOVED***"
client = OpenAI(api_key=openai_api_key)


def load_template(file_path: str) -> str:
    """
    Loads a template from a specified file path.
    
    Args:
        file_path (str): The path to the template file.

    Returns:
        str: The content of the template file.
    """
    with open(file_path, 'r') as file:
        return file.read()


class LLMController:
    def __init__(self):
        # Load the templates from external files
        self.model = "gpt-4o-mini"
        self.location_based_template = load_template("location_based_template.txt")
        self.data_analysis_template = load_template("data_analysis_template.txt")

    def get_response(self, assistant_prompt: str, user_prompt: str) -> str:
        """
        Sends the given prompts to GPT-4 and returns the generated response.
        
        Args:
            assistant_prompt (str): The initial prompt for the assistant.
            user_prompt (str): The prompt/question from the user.

        Returns:
            str: The response text generated by the LLM.
        """
        try:
            messages = [
                {"role": "assistant", "content": assistant_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # Call the OpenAI API to generate a response
            response = client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2,
                max_tokens=500,
                frequency_penalty=0.0
            )
            response_text = response.choices[0].message.content
            tokens_used = response.usage.total_tokens

            return response_text, tokens_used

        except Exception as e:
            raise Exception(f"Error generating response: {str(e)}")

    def handle_location_based_question(self, user_question: str) -> str:
        """
        Process and respond to location-based questions using the location-based template.
        
        Args:
            user_question (str): The question to be processed.

        Returns:
            str: The generated response.
        """
        user_prompt = f"Question: {user_question}\n"
        response, _ = self.get_response(self.location_based_template, user_prompt)
        return response

    def handle_data_analysis_question(self, user_question: str, reasoning: str) -> str:
        """
        Process and respond to questions that require data analysis using the available functions and datasets.
        
        Args:
            user_question (str): The question to be processed.
            reasoning (str): Additional context or reasoning for the question.

        Returns:
            str: The generated response.
        """
        combined_user_prompt = f"Question: {user_question}\n{reasoning}\n"
        response, _ = self.get_response(self.data_analysis_template, combined_user_prompt)
        return response